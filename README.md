# 🤖 Multimodal Large Language Models: Demonstration Notebook

This repository contains a Google Colab notebook demonstrating **multimodal large language models (LLMs)**, capable of processing **both text and images**.

The notebook provides:
- ✅ A hands-on walkthrough of **vision-language transformer models**.
- ✅ Examples of **image captioning, image classification, and visual question answering (VQA)**.
- ✅ Step-by-step explanations of the code and outputs.
- ✅ Usage of **Hugging Face Transformers** and/or **Google PaLI-Gemma 2** for multimodal inference and fine-tuning.

---
## [Video Walkthrough](https://youtu.be/_OS7BDH9Cdk?si=70xFpXm9Y2rhQgbM)

## 🚀 **How to Run**

You can launch the notebook directly on Google Colab:

👉 [Open in Colab](https://colab.research.google.com/drive/1zjaNcJLqqSmJYn8Z6Jqi3t4ATNNp4oWs?usp=sharing)


## 📚 **About This Project**

This project is part of an academic assignment to explore the capabilities and applications of **multimodal LLMs**, inspired by:

- [Hands-On Large Language Models (O'Reilly Book)](https://learning.oreilly.com/library/view/hands-on-large-language/9781098150952/ch09.html)
- [Hands-On LLM GitHub Notebooks](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter09/Chapter%209%20-%20Multimodal%20Large%20Language%20Models.ipynb)
- [Roboflow: Fine-tuning PaLI-Gemma 2](https://blog.roboflow.com/fine-tune-paligemma-2/)

---

## 📝 **Notebook Features**

- 🖼️ Load and process **images** and **text prompts**.
- 🔗 Use **pretrained multimodal models** such as `CLIP`, `BLIP`, or `PaLI-Gemma 2`.
- 🏗️ Optionally **fine-tune** a model on a small dataset.
- 🎯 Perform inference on **caption generation, classification, VQA**.
- 🗣️ Includes **code walkthrough explanations** for each block.

---

## clone this repo:

```bash
https://github.com/Rushabh-Runwal/Multi-modal-language-model
```

## Dependencies
This notebook uses:
transformers
torch
Pillow
datasets
roboflow (if using PaLI-Gemma 2 fine-tuning)


